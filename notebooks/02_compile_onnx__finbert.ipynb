{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to\n",
    "1. Compile a `transformers` model to ONNX format (method is generalizable to custom models)\n",
    "1. Optimize the ONNX model to TensorRT format (which has synergy with Triton Inference Server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to convert a model to ONNX\n",
    "\n",
    "<!-- 1. If supported by `optimum`, use it!\n",
    "1. If a complex torch module, follow best-practice example in `optimum` to compile. -->\n",
    "\n",
    "```.mermaid\n",
    "graph LR\n",
    "    F{Is complex model?}\n",
    "    F -- yes --> A\n",
    "    F -- no --> G[use torch.onnx.export]\n",
    "    A{Is complex model NOT supported by optimum?}\n",
    "    A -- yes --> C[Find most similar model in optimum] --> D[Follow best-practice example]\n",
    "    A -- no --> E[Use optimum]\n",
    "```\n",
    "\n",
    "(An alternative to using `optimum`'s function is to directly use `torch.onnx.export`, but there may be other gotchas that could have been avoided as they are handled by `optimum`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [optimum cotribution guide](https://github.com/huggingface/optimum/blob/d21256c2964945fc3fe4623f7befb21082b69a25/docs/source/exporters/onnx/usage_guides/contribute.mdx#L56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defns & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "from optimum.exporters import TasksManager\n",
    "from optimum.exporters.onnx import export\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from src.config import models_dir\n",
    "from src.utils_compile import (\n",
    "    DefaultConfigInput,\n",
    "    check_model_name,\n",
    "    generate_config_pbtxt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_NAME = \"ProsusAI/finbert\"\n",
    "E2E_MODEL_NAME = check_model_name(\"finbert\")\n",
    "MODEL_NAME = f\"{E2E_MODEL_NAME}-model\"\n",
    "MODEL_VERSION = 1\n",
    "MODEL_DIR = models_dir / f\"{MODEL_NAME}\"\n",
    "MODEL_VERSION_DIR = MODEL_DIR / f\"{MODEL_VERSION}\"\n",
    "PTH_MODEL_ONNX = MODEL_VERSION_DIR / \"model.onnx\"\n",
    "PTH_CONFIG = MODEL_DIR / \"config.pbtxt\"\n",
    "\n",
    "# TensorRT model\n",
    "MODEL_NAME_TRT = f\"{E2E_MODEL_NAME}-trt-model\"\n",
    "MODEL_VERSION_TRT = 1\n",
    "MODEL_TRT_DIR = models_dir / f\"{MODEL_NAME_TRT}\"\n",
    "MODEL_VERSION_TRT_DIR = MODEL_TRT_DIR / f\"{MODEL_VERSION_TRT}\"\n",
    "PTH_MODEL_TRT = MODEL_VERSION_TRT_DIR / \"model.plan\"\n",
    "\n",
    "# Tokenizer \"model\"\n",
    "TOKENIZER_MODEL_DIR = models_dir / f\"{E2E_MODEL_NAME}-tokenizer\"\n",
    "TOKENIZER_MODEL_VERSION = 1\n",
    "TOKENIZER_MODEL_VERSION_DIR = TOKENIZER_MODEL_DIR / f\"{TOKENIZER_MODEL_VERSION}\"\n",
    "PTH_TOKENIZER_MODEL_DATA = TOKENIZER_MODEL_VERSION_DIR / \"tokenizer_data\"\n",
    "\n",
    "# End-to-end model (pipeline model; a.k.a. \"ensemble\" model in Nvidia terminology)\n",
    "E2E_MODEL_DIR = models_dir / E2E_MODEL_NAME\n",
    "E2E_PTH_CONFIG = E2E_MODEL_DIR / \"config.pbtxt\"\n",
    "\n",
    "\n",
    "MODEL_VERSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_VERSION_TRT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TOKENIZER_MODEL_VERSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "E2E_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# onnx_opset = onnx_config.DEFAULT_ONNX_OPSET\n",
    "onnx_opset = 20  # highest supported opset version by `torch.onnx.export()` for torch==2.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Torch model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(and save tokenizer as \"model\" too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 2.5.1+cu124\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running inference via ONNX runtime\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(HF_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "\n",
    "## Save tokenizer as a \"model\"\n",
    "\n",
    "tokenizer.save_pretrained(PTH_TOKENIZER_MODEL_DATA)\n",
    "\n",
    "\n",
    "## Compile the ONNX model\n",
    "\n",
    "onnx_config_constructor = TasksManager.get_exporter_config_constructor(\n",
    "    \"onnx\",\n",
    "    model,\n",
    "    task=\"text-classification\",  # NOTE: change to others where applicable e.g. \"summarization\" (Refer to: `TasksManager.get_all_tasks()`)\n",
    "    library_name=\"transformers\",  # NOTE: change to others where applicable e.g. \"sentence_transformers\"\n",
    ")\n",
    "onnx_config = onnx_config_constructor(model.config)\n",
    "\n",
    "onnx_config.int_dtype = \"int32\"  # We force int32 since it is unlikely any of {input_ids, attention_mask, token_type_ids} will have values > 2^31\n",
    "\n",
    "inputs = tokenizer(\"Stocks rallied and the British pound gained.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs_orig = model(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"])\n",
    "\n",
    "\n",
    "onnx_inputs, onnx_outputs = export(model, onnx_config, PTH_MODEL_ONNX, opset=onnx_opset)\n",
    "\n",
    "\n",
    "print(\"running inference via ONNX runtime\")\n",
    "\n",
    "\n",
    "ort_session = ort.InferenceSession(PTH_MODEL_ONNX)\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\n",
    "        \"input_ids\": inputs[\"input_ids\"].numpy().astype(np.int32),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].numpy().astype(np.int32),\n",
    "        \"token_type_ids\": inputs[\"token_type_ids\"].numpy().astype(np.int32),\n",
    "    },\n",
    ")\n",
    "assert ((outputs[0] - outputs_orig.logits.numpy()) < 1e-3).all()\n",
    "\n",
    "# print(f\"logits: {outputs_orig.logits}\")\n",
    "# print(f\"probability (pos, neg, neutral): {torch.softmax(outputs_orig.logits, dim=1)}\")\n",
    "\n",
    "# PTH_MODEL_ONNX.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [WIP] Auto-generate `config.pbtxt` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate config.pbtxt\n",
    "# TODO Automation still work in progress\n",
    "# _ = generate_config_pbtxt(\n",
    "#     DefaultConfigInput(\n",
    "#         model_name=MODEL_NAME,\n",
    "#         max_batch_size=16,  # MUST be >= preferred_batch_size\n",
    "#         preferred_batch_size=[8, 16],\n",
    "#         max_queue_delay_microseconds=100,\n",
    "#     ),\n",
    "#     pth_config=PTH_CONFIG,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile ONNX to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "# Logger for TensorRT\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "MAX_BATCH_SIZE = 16  # Maximum batch size for inference\n",
    "MIN_SEQUENCE_LENGTH = 8  # Smallest valid sequence length\n",
    "OPT_SEQUENCE_LENGTH = 128  # Typical sequence length\n",
    "MAX_SEQUENCE_LENGTH = 512  # Maximum valid sequence length\n",
    "FP16_MODE = True  # Enable FP16 precision (if supported)\n",
    "INT8_MODE = False  # Enable INT8 precision (if calibration data is available)\n",
    "WORKSPACE_SIZE = 1 << 30  # 1GB workspace size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set profile for input_ids: Min=(1, 8), Opt=(8, 128), Max=(16, 512)\n",
      "Set profile for attention_mask: Min=(1, 8), Opt=(8, 128), Max=(16, 512)\n",
      "Set profile for token_type_ids: Min=(1, 8), Opt=(8, 128), Max=(16, 512)\n",
      "Building TensorRT serialized engine. This may take a while...\n",
      "Serialized engine built successfully!\n",
      "Engine saved at /home/ss/work/test-triton/models/finbert-trt-model/1/model.plan\n",
      "Engine deserialized successfully!\n"
     ]
    }
   ],
   "source": [
    "## Builds a TensorRT engine from an ONNX model with dynamic shape support\n",
    "\n",
    "with (\n",
    "    trt.Builder(TRT_LOGGER) as builder,\n",
    "    builder.create_network(trt.NetworkDefinitionCreationFlag.STRONGLY_TYPED) as network,\n",
    "    trt.OnnxParser(network, TRT_LOGGER) as parser,\n",
    "):\n",
    "\n",
    "    # Configure builder\n",
    "    config = builder.create_builder_config()\n",
    "    try:\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, WORKSPACE_SIZE)\n",
    "    except AttributeError:\n",
    "        config.max_workspace_size = WORKSPACE_SIZE\n",
    "\n",
    "    if FP16_MODE:\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "    if INT8_MODE:\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "\n",
    "    # Parse ONNX model\n",
    "    with PTH_MODEL_ONNX.open(\"rb\") as model:\n",
    "        if not parser.parse(model.read()):\n",
    "            print(\"Failed to parse the ONNX file:\")\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "\n",
    "    # Handle dynamic shapes with an optimization profile\n",
    "    profile = builder.create_optimization_profile()\n",
    "    dynamic_inputs = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "\n",
    "    for input_name in dynamic_inputs:\n",
    "        input_tensor = network.get_input(dynamic_inputs.index(input_name))\n",
    "        tensor_shape = input_tensor.shape\n",
    "\n",
    "        # Handle dynamic dimensions\n",
    "        if tensor_shape[0] == -1:\n",
    "            profile.set_shape(\n",
    "                input_name,\n",
    "                (1, MIN_SEQUENCE_LENGTH),  # Min: Batch=1, Min Seq Len\n",
    "                (MAX_BATCH_SIZE // 2, OPT_SEQUENCE_LENGTH),  # Opt: Half Batch, Opt Seq Len\n",
    "                (MAX_BATCH_SIZE, MAX_SEQUENCE_LENGTH),  # Max: Full Batch, Max Seq Len\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Warning: Input {input_name} does not have dynamic dimensions.\")\n",
    "\n",
    "        print(\n",
    "            f\"Set profile for {input_name}: Min=(1, {MIN_SEQUENCE_LENGTH}), \"\n",
    "            f\"Opt=({MAX_BATCH_SIZE // 2}, {OPT_SEQUENCE_LENGTH}), \"\n",
    "            f\"Max=({MAX_BATCH_SIZE}, {MAX_SEQUENCE_LENGTH})\"\n",
    "        )\n",
    "\n",
    "    config.add_optimization_profile(profile)\n",
    "\n",
    "    # Build the serialized engine\n",
    "    print(\"Building TensorRT serialized engine. This may take a while...\")\n",
    "    serialized_engine = builder.build_serialized_network(network, config)\n",
    "    if serialized_engine:\n",
    "        print(\"Serialized engine built successfully!\")\n",
    "        with open(PTH_MODEL_TRT, \"wb\") as f:\n",
    "            f.write(serialized_engine)\n",
    "        print(f\"Engine saved at {PTH_MODEL_TRT}\")\n",
    "    else:\n",
    "        print(\"Failed to build the serialized engine.\")\n",
    "\n",
    "    # Deserialize engine for verification (Optional)\n",
    "    runtime = trt.Runtime(TRT_LOGGER)\n",
    "    engine = runtime.deserialize_cuda_engine(serialized_engine)\n",
    "    if engine:\n",
    "        print(\"Engine deserialized successfully!\")\n",
    "    else:\n",
    "        print(\"Failed to deserialize engine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
